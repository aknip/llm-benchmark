{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfZrHxIT+jiaDtnKG854WS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aknip/llm-benchmark/blob/main/llm_simple_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O1ck0sc0m2v"
      },
      "outputs": [],
      "source": [
        "!pip install openai tiktoken litellm pandas openpyxl bs4 icecream --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from getpass import getpass\n",
        "import psutil\n",
        "import requests\n",
        "import textwrap\n",
        "from icecream import ic\n",
        "import time\n",
        "IN_NOTEBOOK = any([\"jupyter-notebook\" in i for i in psutil.Process().parent().cmdline()])\n",
        "if IN_NOTEBOOK:\n",
        "  CREDS = json.loads(getpass(\"Secrets (JSON string): \"))\n",
        "  os.environ['CREDS'] = json.dumps(CREDS)\n",
        "  CREDS = json.loads(os.getenv('CREDS'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myZYsW9oZkEv",
        "outputId": "92c3b730-93f2-4d7f-ff4a-4d3e67dd8b6b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Secrets (JSON string): ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from litellm import completion\n",
        "os.environ[\"OPENAI_API_KEY\"] = CREDS['OpenAI']['v1']['credential'] # my key\n",
        "os.environ[\"TOGETHERAI_API_KEY\"] = CREDS['together-ai']['key']['credential']"
      ],
      "metadata": {
        "id": "y0pi_2z0ZsOS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "pigCauDiDksx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "client = openai.OpenAI(\n",
        "  api_key=os.environ.get(\"TOGETHERAI_API_KEY\"),\n",
        "  base_url='https://api.together.xyz',\n",
        ")\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Hello, how are you?\",\n",
        "    }\n",
        "  ],\n",
        "  model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "  max_tokens=1024\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku-bklBeaTpI",
        "outputId": "7128f3a0-37c0-4032-dbc4-1c6c3df610db"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I'm just a computer program, so I don't have feelings, but I'm here to help you with any language-related questions you have. Is there something specific you would like to know or practice?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Call via litellm"
      ],
      "metadata": {
        "id": "E_gDWvF6c1jp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = completion(\n",
        "  #model=\"together_ai/togethercomputer/Llama-2-7B-32K-Instruct\",\n",
        "  model=\"together_ai/mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "  messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoaFpFXKc8zE",
        "outputId": "834ec820-9631-4788-bf8d-530c9a7fbf2c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to assist you with any questions you have. Is there something specific you would like to know or talk about?\n",
            "ModelResponse(id='chatcmpl-80ba4d38-ee9f-42f1-b2e4-71c4db0f487e', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to assist you with any questions you have. Is there something specific you would like to know or talk about?\", role='assistant'))], created=1703926174, model='together_ai/mistralai/Mixtral-8x7B-Instruct-v0.1', object='chat.completion', system_fingerprint=None, usage=Usage(prompt_tokens=12, completion_tokens=44, total_tokens=56), _response_ms=2074.8419999999996)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = completion(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4QNQHwhc38u",
        "outputId": "0a0e02e8-51fa-45ca-d5e0-f0f62ba9745d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm an AI, so I don't have feelings, but I'm here to assist you. How can I help you today?\n",
            "ModelResponse(id='chatcmpl-8bPKKlY5fJEniH6li1bSHil8wYqAh', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"I'm an AI, so I don't have feelings, but I'm here to assist you. How can I help you today?\", role='assistant'))], created=1703926183, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=Usage(completion_tokens=27, prompt_tokens=13, total_tokens=40), _response_ms=1607.078)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function Calling"
      ],
      "metadata": {
        "id": "XJyLwl3CjYOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# see https://litellm.vercel.app/docs/completion/function_call\n",
        "\n",
        "# via Huggingface?\n",
        "# https://litellm.vercel.app/docs/providers/huggingface\n",
        "# https://huggingface.co/Trelis/Mixtral-8x7B-Instruct-v0.1-function-calling-v3\n",
        "# https://huggingface.co/Trelis/Mistral-7B-Instruct-v0.1-function-calling-v2\n",
        "\n",
        "# via Anyscale?\n",
        "# https://docs.litellm.ai/docs/providers/anyscale\n",
        "# https://www.anyscale.com/blog/anyscale-endpoints-json-mode-and-function-calling-features"
      ],
      "metadata": {
        "id": "lD6O-SQxjbCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, litellm\n",
        "from litellm import completion\n",
        "\n",
        "# IMPORTANT - Set this to TRUE to add the function to the prompt for Non OpenAI LLMs\n",
        "litellm.add_function_to_prompt = True\n",
        "\n",
        "# The real function is not needed for the LLM. It may be called after the LLM call (not in this code!)\n",
        "def get_current_weather(location):\n",
        "  if location == \"Boston, MA\":\n",
        "    return \"The weather is 12F\"\n",
        "\n",
        "functions = [\n",
        "    {\n",
        "      \"name\": \"get_current_weather\",\n",
        "      \"description\": \"Get the current weather in a given location\",\n",
        "      \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "          \"location\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
        "          },\n",
        "          \"unit\": {\n",
        "            \"type\": \"string\",\n",
        "            \"enum\": [\"celsius\", \"fahrenheit\"]\n",
        "          }\n",
        "        },\n",
        "        \"required\": [\"location\"]\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is the weather like in Boston?\"}\n",
        "]\n",
        "\n",
        "response = completion(model=\"gpt-3.5-turbo-1106\", messages=messages, functions=functions)\n",
        "\n",
        "print(response)\n",
        "print()\n",
        "function_found = hasattr(response.choices[0]['message'], 'function_call')\n",
        "if function_found == True:\n",
        "  function_call = response.choices[0]['message']['function_call']\n",
        "  function_call_name = function_call.name\n",
        "  function_call_arguments = function_call.arguments\n",
        "  print(function_call_name)\n",
        "else:\n",
        "  print('No function found')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TG6qj6W282Dz",
        "outputId": "03117e1e-99d0-4550-a0f5-7ffb14c908e9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModelResponse(id='chatcmpl-8bPKfEySA6R1vl8rdfdB9cFJdphsh', choices=[Choices(finish_reason='function_call', index=0, message=Message(content=None, role='assistant', function_call=FunctionCall(arguments='{\"location\":\"Boston, MA\"}', name='get_current_weather')))], created=1703926205, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_772e8125bb', usage=Usage(completion_tokens=17, prompt_tokens=82, total_tokens=99), _response_ms=998.558)\n",
            "\n",
            "get_current_weather\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Create Excel file"
      ],
      "metadata": {
        "id": "qMx9SDXex-CK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import openpyxl\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "\n",
        "# see https://openpyxl.readthedocs.io/en/2.4/pandas.html\n",
        "\n",
        "# Create a Pandas dataframe from the data.\n",
        "df = pd.DataFrame({'Name': ['Miller', 'Adams', 'Smith'],\n",
        "                   'Prompt': ['Write something', 'Do different things', 'Summarize it'], })\n",
        "\n",
        "# Create a Pandas Excel writer using openpyxl as the engine.\n",
        "writer = pd.ExcelWriter('llm_benchmark.xlsx', engine='openpyxl')\n",
        "\n",
        "# Convert the dataframe to an XlsxWriter Excel object.\n",
        "df.to_excel(writer, sheet_name='Benchmark', index=True)\n",
        "\n",
        "# Get the xlsxwriter objects from the dataframe writer object.\n",
        "workbook  = writer.book\n",
        "worksheet = writer.sheets['Benchmark']\n",
        "\n",
        "# Save the Excel file.\n",
        "workbook.save('llm_benchmark.xlsx')\n",
        "\n",
        "# Alternate approach without ExcelWriter:\n",
        "# workbook = Workbook()\n",
        "# worksheet = workbook.active\n",
        "# for row in dataframe_to_rows(df, index=True, header=True):\n",
        "#     if row != [None]:\n",
        "#       worksheet.append(row)"
      ],
      "metadata": {
        "id": "fRhw6GLCxQLL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Read Excel file as dataframe"
      ],
      "metadata": {
        "id": "aPYRadlYz2v1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#import xlrd\n",
        "\n",
        "df = pd.read_excel('llm_benchmark.xlsx', sheet_name='Benchmark') # parse_dates=['date'] # dtype={'column_name': float}\n",
        "#print(df.head())\n",
        "\n",
        "print(\"Given Dataframe :\\n\", df)\n",
        "\n",
        "print(\"\\nIterating 1:\")\n",
        "for i in df.index:\n",
        "    print(i, df['Name'][i],df['Prompt'][i])\n",
        "\n",
        "print(\"\\nIterating 2:\")\n",
        "for index, row in df.iterrows():\n",
        "    name = row['Name']\n",
        "    prompt = row['Prompt']\n",
        "    print(f\"{index}: {name}, {prompt}\")"
      ],
      "metadata": {
        "id": "eQ-GM5MqzMwQ",
        "outputId": "b5a7a8c8-984d-4187-a99c-80a56f26285b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given Dataframe :\n",
            "    Unnamed: 0    Name               Prompt\n",
            "0           0  Miller      Write something\n",
            "1           1   Adams  Do different things\n",
            "2           2   Smith         Summarize it\n",
            "\n",
            "Iterating 1:\n",
            "0 Miller Write something\n",
            "1 Adams Do different things\n",
            "2 Smith Summarize it\n",
            "\n",
            "Iterating 2:\n",
            "0: Miller, Write something\n",
            "1: Adams, Do different things\n",
            "2: Smith, Summarize it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Updata data and Excel file"
      ],
      "metadata": {
        "id": "jnRHMJPx16hM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.at[1, 'Name'] = 'Name-Updated'\n",
        "df.at[2, 'Prompt'] = 'Prompt-Updated'\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "c7kIjqA22Oy_",
        "outputId": "37731d4c-099a-48e2-ceb0-7af30a1b87ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0          Name               Prompt\n",
            "0           0        Miller      Write something\n",
            "1           1  Name-Updated  Do different things\n",
            "2           2         Smith       Prompt-Updated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import openpyxl\n",
        "from openpyxl import Workbook\n",
        "from openpyxl import load_workbook\n",
        "workbook = load_workbook(filename = 'llm_benchmark.xlsx')\n",
        "worksheet = workbook.active\n",
        "\n",
        "print(\"Updating Excel file:\")\n",
        "for index, row in df.iterrows():\n",
        "    name = row['Name']\n",
        "    prompt = row['Prompt']\n",
        "    print(f\"{index}: {name}, {prompt}\")\n",
        "    d = worksheet.cell(row=index+2, column=2, value=name)\n",
        "    d = worksheet.cell(row=index+2, column=3, value=prompt)\n",
        "\n",
        "# Alternative: delete data rows and append dataframe to worksheet\n",
        "# continuously delete row 2 until there is only first row (header row) is left over\n",
        "# while(worksheet.max_row > 1):\n",
        "#     worksheet.delete_rows(2) # removes the row 2\n",
        "# append dataframe to worksheet\n",
        "# for row in dataframe_to_rows(df, index=False, header=False):\n",
        "#     if row != [None]:\n",
        "#        worksheet.append(row)\n",
        "\n",
        "# Save the Excel file.\n",
        "workbook.save('llm_benchmark.xlsx')"
      ],
      "metadata": {
        "id": "SCUeSs4N2nQV",
        "outputId": "880cd1f7-c5ec-4657-e26d-2d06aaaa7e3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating Excel file:\n",
            "0: Miller, Write something\n",
            "1: Name-Updated, Do different things\n",
            "2: Smith, Prompt-Updated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create file, read file, save to Excel"
      ],
      "metadata": {
        "id": "fuhYzmz48FlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "email_text = '''Enter some text here\n",
        "and here\n",
        "and here\n",
        "'''"
      ],
      "metadata": {
        "id": "j9j9fjuw8n3I"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f= open('24-anonymized-shortened.txt','w+')\n",
        "f.write(email_text)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "Y9WP3vyz9d-f"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "f= open('24-anonymized-shortened.txt','r')\n",
        "if f.mode == 'r':\n",
        "      email_text =f.read()\n",
        "      #print(email_text)\n",
        "f.close()\n",
        "\n",
        "email_text_tokens = len(tiktoken.get_encoding('cl100k_base').encode(email_text))"
      ],
      "metadata": {
        "id": "E6TfVxg68HM8"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import openpyxl\n",
        "from openpyxl import Workbook\n",
        "from openpyxl import load_workbook\n",
        "workbook = load_workbook(filename = 'llm_benchmark.xlsx')\n",
        "worksheet = workbook.active\n",
        "\n",
        "print(\"Writing email-text to Excel file...\")\n",
        "worksheet.cell(row=5, column=1, value=3)\n",
        "worksheet.cell(row=5, column=2, value=\"mytest\")\n",
        "worksheet.cell(row=5, column=3, value=email_text)\n",
        "worksheet.cell(row=5, column=4, value=email_text_tokens)\n",
        "workbook.save('llm_benchmark.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0vvggA597dd",
        "outputId": "d1f6bade-9ae9-4aec-fc83-3d006155f5e9"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing email-text to Excel file...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read email from Excel"
      ],
      "metadata": {
        "id": "jmCRUjfR_NrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel('llm_benchmark.xlsx', sheet_name='Benchmark') # parse_dates=['date'] # dtype={'column_name': float}\n",
        "email_text = df['Prompt'][3]\n",
        "print(df.head())\n",
        "#print(df['Name'][3],df['Prompt'][3][:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tK0kxHV9_RjP",
        "outputId": "81826d5c-e70b-49b2-ebcf-c788c609fa76"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0    Name                                             Prompt  \\\n",
            "0           0  Miller                                    Write something   \n",
            "1           1   Adams                                Do different things   \n",
            "2           2   Smith                                       Summarize it   \n",
            "3           3  mytest  # Email Nachricht:\\n\\nFrom: Ralf Löffler - Uni...   \n",
            "\n",
            "   Unnamed: 3  \n",
            "0         NaN  \n",
            "1         NaN  \n",
            "2         NaN  \n",
            "3      2677.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "email_text_tokens = len(tiktoken.get_encoding('cl100k_base').encode(email_text))\n",
        "print(email_text_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CabKtm3UBzuk",
        "outputId": "e4a664db-57e4-4d1d-aa32-aeec77225ff2"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt1 = '''Nachfolgend sende ich alle Informationen für eine Anfrage eines Versicherungsmaklers für eine D&O-Versicherung seines Kunden. Die Informationen bestehen aus:\n",
        "- Email Nachricht\n",
        "- Organigramm\n",
        "- Versicherungsschein\n",
        "- Nachtrag\n",
        "- Versicherungsbedingungen\n",
        "'''"
      ],
      "metadata": {
        "id": "KWTkkoT8EsOu"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt2 = 'Bitte bestätige mit \"Ja\" wenn Du die Inhalte verstanden hast.'"
      ],
      "metadata": {
        "id": "GNaI8mp7E1YS"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt3 = 'Wie hoch ist die Versicherungssumme? Antworte mit folgendem JSON-Schema: {\"Versicherungssumme\": \"\"}, formatiere Zahlen in europäischer Notation.'"
      ],
      "metadata": {
        "id": "OBVatqSJE5x5"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_prompt = prompt1 + '\\n\\n' + prompt3 + '\\n\\n' + email_text"
      ],
      "metadata": {
        "id": "uhBKJKvNI0kz"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = completion(\n",
        "  #model=\"together_ai/togethercomputer/llama-2-70b\",\n",
        "  #model=\"together_ai/togethercomputer/llama-2-70b-chat\",\n",
        "  #model=\"together_ai/togethercomputer/Llama-2-7B-32K-Instruct\",\n",
        "  model=\"together_ai/mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "  #model=\"gpt-3.5-turbo\",\n",
        "  messages=[{ \"content\": full_prompt,\"role\": \"user\"}],\n",
        "  #temperature=0.5,\n",
        "  max_tokens=100\n",
        ")\n",
        "\n",
        "print(response)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2gfZrs-E-1S",
        "outputId": "d3a1a2c2-7b2e-4474-d703-58b3f05b3631"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModelResponse(id='chatcmpl-a14a4efb-9586-46c2-a7b8-6f9b87b1a9df', choices=[Choices(finish_reason='stop', index=0, message=Message(content='{\\n\"Versicherungssumme\": \"5.500.000,00 €\"\\n}', role='assistant'))], created=1703938939, model='together_ai/mistralai/Mixtral-8x7B-Instruct-v0.1', object='chat.completion', system_fingerprint=None, usage=Usage(prompt_tokens=2779, completion_tokens=20, total_tokens=2799), _response_ms=2487.099)\n",
            "{\n",
            "\"Versicherungssumme\": \"5.500.000,00 €\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "response = requests.get(\"https://www.tagesschau.de/inland/hochwasser-deutschland-126.html\")\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "all_text = soup.get_text(separator='\\n', strip=True)\n",
        "div_text=soup.find(\"div\",{\"class\":\"layout-container\"}).get_text()\n",
        "article_text=soup.find(\"article\").get_text(separator='\\n', strip=True)\n",
        "print(article_text)"
      ],
      "metadata": {
        "id": "CqF77kMri2XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = completion(\n",
        "  #model=\"together_ai/togethercomputer/llama-2-70b\",\n",
        "  #model=\"together_ai/togethercomputer/llama-2-70b-chat\",\n",
        "  #model=\"together_ai/togethercomputer/Llama-2-7B-32K-Instruct\",\n",
        "  model=\"together_ai/mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "  #model=\"gpt-3.5-turbo\",\n",
        "  messages=[{ \"content\": \"Summarize the following text in 3 sentences, answer in German language: \" + article_text,\"role\": \"user\"}],\n",
        "  #temperature=0.5,\n",
        "  max_tokens=500\n",
        ")\n",
        "\n",
        "print(response)\n",
        "print(textwrap.fill(response.choices[0].message.content, 80))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XubC1GR1nTlW",
        "outputId": "8d853453-cba6-4e81-bfd0-e6aa7212e129"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModelResponse(id='chatcmpl-256bdced-aaf7-4b16-ba09-76d0b1c71d8d', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Die Hochwasserlage in Niedersachsen ist weiter kritisch, insbesondere in den Landkreisen Celle, Heidekreis, Verden, Emsland und Oldenburg. Obwohl sich die Lage in einigen Gebieten entspannt hat, steigen die Pegelstände an anderen Orten noch. Das THW ist auf einen Einsatz in den Hochwassergebieten bis in die erste Januarwoche hinein vorbereitet. Die Deiche sind massiv aufgeweicht, was große Sorge bereitet. Das Land Niedersachsen ist mit Rettungskräften gut aufgestellt und geht davon aus, die Lage auch über Silvester mit eigenen Kräften bewältigen zu können.', role='assistant'))], created=1703939215, model='together_ai/mistralai/Mixtral-8x7B-Instruct-v0.1', object='chat.completion', system_fingerprint=None, usage=Usage(prompt_tokens=2376, completion_tokens=160, total_tokens=2536), _response_ms=3085.318)\n",
            "Die Hochwasserlage in Niedersachsen ist weiter kritisch, insbesondere in den\n",
            "Landkreisen Celle, Heidekreis, Verden, Emsland und Oldenburg. Obwohl sich die\n",
            "Lage in einigen Gebieten entspannt hat, steigen die Pegelstände an anderen Orten\n",
            "noch. Das THW ist auf einen Einsatz in den Hochwassergebieten bis in die erste\n",
            "Januarwoche hinein vorbereitet. Die Deiche sind massiv aufgeweicht, was große\n",
            "Sorge bereitet. Das Land Niedersachsen ist mit Rettungskräften gut aufgestellt\n",
            "und geht davon aus, die Lage auch über Silvester mit eigenen Kräften bewältigen\n",
            "zu können.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Do Benchmark"
      ],
      "metadata": {
        "id": "mm371_jG5xU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import textwrap\n",
        "from datetime import datetime\n",
        "import openpyxl\n",
        "from openpyxl import Workbook\n",
        "from openpyxl import load_workbook\n",
        "import pandas as pd\n",
        "import json\n",
        "from io import StringIO\n",
        "\n",
        "# load input file\n",
        "workbook = load_workbook(filename = 'llm_benchmark.xlsx', data_only=True) # data_only=True\n",
        "worksheet_emails = workbook['Emails'] # workbook.active\n",
        "worksheet_llm_config = workbook['LLM-Configs']\n",
        "\n",
        "# create log file (based on input file, sheet 'Results', all other sheets are deleted)\n",
        "workbook_log = load_workbook(filename = 'llm_benchmark.xlsx')\n",
        "workbook_log.remove(workbook_log['README'])\n",
        "workbook_log.remove(workbook_log['Emails'])\n",
        "workbook_log.remove(workbook_log['LLM-Configs'])\n",
        "worksheet_log = workbook_log['Results']\n",
        "while(worksheet_log.max_row > 1): # delete log rows: continuously delete row 2 until there is only first row (header row) left over\n",
        "  worksheet_log.delete_rows(2)\n",
        "\n",
        "# create headers dictionaries to access columns\n",
        "column_email = {}\n",
        "for column_index, cell in enumerate(worksheet_emails[\"1\"]):\n",
        "  column_email[cell.value] = column_index + 1\n",
        "column_llm_config = {}\n",
        "for column_index, cell in enumerate(worksheet_llm_config[\"1\"]):\n",
        "  column_llm_config[cell.value] = column_index + 1\n",
        "\n",
        "# iterate through all lines\n",
        "for row_index, row1 in enumerate(worksheet_emails):\n",
        "  if row_index == 12: # > 0  //  == 12\n",
        "    dateiname = worksheet_emails.cell(row=row_index+1, column=column_email[\"Dateiname\"]).value\n",
        "    if dateiname != None:\n",
        "      ic(dateiname)\n",
        "      prompt_context = worksheet_emails.cell(row=row_index+1, column=column_email[\"Prompt-Context\"]).value\n",
        "      target_JSON = worksheet_emails.cell(row=row_index+1, column=column_email[\"Ziel-JSON\"]).value\n",
        "      llm_configs = worksheet_emails.cell(row=row_index+1, column=column_email[\"LLM-Configs\"]).value.split(',')\n",
        "\n",
        "      for config_index, config in enumerate(llm_configs):\n",
        "        # search config\n",
        "        found_flag = False\n",
        "        for llm_config_index, row2 in enumerate(worksheet_llm_config):\n",
        "          llm_config_id = worksheet_llm_config.cell(row=llm_config_index+1, column=column_llm_config[\"LLM-Config-ID\"]).value\n",
        "          if llm_config_id.strip() == config.strip():\n",
        "            found_flag = True\n",
        "            break\n",
        "        if found_flag == True:\n",
        "          ic('LLM-Config found:', dateiname, config)\n",
        "          llm_config_LLM_Model = worksheet_llm_config.cell(row=llm_config_index+1, column=column_llm_config[\"LLM-Model\"]).value\n",
        "          llm_config_Prompt = worksheet_llm_config.cell(row=llm_config_index+1, column=column_llm_config[\"Prompt\"]).value\n",
        "          llm_config_Response_Format = worksheet_llm_config.cell(row=llm_config_index+1, column=column_llm_config[\"Response-Format\"]).value\n",
        "          prompt_full = llm_config_Prompt + '\\n\\n' + prompt_context\n",
        "          prompt_full_tokenlength = len(tiktoken.get_encoding('cl100k_base').encode(prompt_full))\n",
        "          #for x in prompt_full[:500].split('\\n'):\n",
        "          #  print(textwrap.fill(x, 80))\n",
        "\n",
        "          #\n",
        "          # do LLM call here\n",
        "          #\n",
        "          start_time = time.time()\n",
        "          response = completion(\n",
        "            #model=\"together_ai/mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "            model=llm_config_LLM_Model,\n",
        "            messages=[{ \"content\": prompt_full,\"role\": \"user\"}],\n",
        "            #temperature=0.5,\n",
        "            max_tokens=500\n",
        "          )\n",
        "          response_txt = response.choices[0].message.content\n",
        "          response_JSON = ''\n",
        "          if llm_config_Response_Format == \"JSON\":\n",
        "            response_JSON = response_txt\n",
        "          else:\n",
        "            if llm_config_Response_Format == \"CSV\":\n",
        "              response_IO = StringIO(response_txt) # [4:] strip away first 4 chars \"AI: \"\n",
        "              response_df = pd.read_csv(response_IO, sep=\"|\")\n",
        "              response_JSON_Obj = json.loads(response_df.to_json(orient='table',index=False))['data'][0]\n",
        "              response_JSON = json.dumps(response_JSON_Obj) # ,indent=4\n",
        "\n",
        "          response_model = response.model\n",
        "          response_responsetime = float(response._response_ms)/1000\n",
        "          time.sleep(1)\n",
        "          #response = \"response-obj\"\n",
        "          #response_txt = \"dummy text\"\n",
        "          stop_time = time.time()\n",
        "          elapsed_time = stop_time-start_time\n",
        "\n",
        "          # log result\n",
        "          current_date_time = datetime.now().strftime(\"%d.%m.%Y %H:%M:%S\")\n",
        "          # Time\tEmail-Name\n",
        "          log_row = [current_date_time, # Time\n",
        "                     dateiname,         # Email-Name\n",
        "                     config.strip(),    # LLM-Config-ID\n",
        "                     llm_config_LLM_Model, # Input-Model\n",
        "                     prompt_full,       # Input-Full-Prompt\n",
        "                     prompt_context,    # Input-Context\n",
        "                     str(response),     # Response-Full\n",
        "                     response_model,    # Response-Model\n",
        "                     response_responsetime, # Response-Duration\n",
        "                     elapsed_time,      # Duration-end-to-end\n",
        "                     response_txt,      # Ergebnis\n",
        "                     response_JSON,     # Ergebnis-JSON\n",
        "                     target_JSON         # Ziel-JSON\n",
        "\n",
        "          ]\n",
        "          worksheet_log.append(log_row)\n",
        "        else:\n",
        "          # LLM-Config not found - log result\n",
        "          ic('LLM-Config not found:', dateiname, config)\n",
        "          current_date_time = datetime.now().strftime(\"%d.%m.%Y %H:%M:%S\")\n",
        "          log_row = [current_date_time, dateiname,'NOT FOUND: ' + config.strip()]\n",
        "          worksheet_log.append(log_row)\n",
        "\n",
        "# Save the log file with timestamp\n",
        "current_date_time = '_' + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
        "current_date_time = \"\" # for debugging\n",
        "workbook_log.save('llm_benchmark_log' + current_date_time + '.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4TymQFZJw3V",
        "outputId": "9d3297c2-623e-4b4a-dbb0-91ffc8411f04"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ic| dateiname: '24optimized.msg'\n",
            "ic| 'LLM-Config found:': 'LLM-Config found:'\n",
            "    dateiname: '24optimized.msg'\n",
            "    config: 'Simple_Prompt_1_GPT3_5'\n",
            "ic| 'LLM-Config found:': 'LLM-Config found:'\n",
            "    dateiname: '24optimized.msg'\n",
            "    config: ' Simple_Prompt_1_GPT4'\n",
            "ic| 'LLM-Config found:': 'LLM-Config found:'\n",
            "    dateiname: '24optimized.msg'\n",
            "    config: ' KOR_Prompt_1_GPT3_5'\n",
            "ic| 'LLM-Config found:': 'LLM-Config found:'\n",
            "    dateiname: '24optimized.msg'\n",
            "    config: ' KOR_Prompt_2_GPT4'\n",
            "ic| 'LLM-Config not found:': 'LLM-Config not found:'\n",
            "    dateiname: '24optimized.msg'\n",
            "    config: ' wrong_config '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openpyxl\n",
        "from openpyxl import Workbook\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "# load log file\n",
        "workbook_log = load_workbook(filename = 'llm_benchmark_log.xlsx') # data_only=True\n",
        "worksheet_log = workbook_log['Results']\n",
        "\n",
        "# create headers dictionaries to access columns\n",
        "column_log = {}\n",
        "for column_index, cell in enumerate(worksheet_log[\"1\"]):\n",
        "  column_log[cell.value] = column_index + 1\n",
        "ic(column_index)\n",
        "\n",
        "# iterate through all lines\n",
        "for log_index, row3 in enumerate(worksheet_log):\n",
        "  if log_index == 1: # > 0  //  == 1\n",
        "    log_time = worksheet_log.cell(row=log_index+1, column=column_log[\"Time\"]).value\n",
        "    ic('found', log_time)"
      ],
      "metadata": {
        "id": "Z2GGmbER3HVi",
        "outputId": "ba25be06-f676-4a82-ad83-0e0ad5b136bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ic| column_index: 12\n",
            "ic| 'found', log_time: '02.01.2024 15:35:52'\n"
          ]
        }
      ]
    }
  ]
}