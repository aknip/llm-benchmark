{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHokAyY3Wuu0VzJC6I0N84",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aknip/llm-benchmark/blob/main/llm_simple_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O1ck0sc0m2v"
      },
      "outputs": [],
      "source": [
        "!pip install openai tiktoken litellm pandas openpyxl icecream --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from getpass import getpass\n",
        "import psutil\n",
        "import requests\n",
        "import textwrap\n",
        "from icecream import ic\n",
        "import time\n",
        "IN_NOTEBOOK = any([\"jupyter-notebook\" in i for i in psutil.Process().parent().cmdline()])\n",
        "if IN_NOTEBOOK:\n",
        "  CREDS = json.loads(getpass(\"Secrets (JSON string): \"))\n",
        "  os.environ['CREDS'] = json.dumps(CREDS)\n",
        "  CREDS = json.loads(os.getenv('CREDS'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myZYsW9oZkEv",
        "outputId": "211be57b-bc71-4129-d32e-1063187e42e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Secrets (JSON string): ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from litellm import completion\n",
        "os.environ[\"OPENAI_API_KEY\"] = CREDS['OpenAI']['v1']['credential'] # my key\n",
        "os.environ[\"TOGETHERAI_API_KEY\"] = CREDS['together-ai']['key']['credential']"
      ],
      "metadata": {
        "id": "y0pi_2z0ZsOS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run benchmark\n",
        "\n",
        "Excel file \"llm_benchmark.xlsx\" must be in the same folder!"
      ],
      "metadata": {
        "id": "mm371_jG5xU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import textwrap\n",
        "from datetime import datetime\n",
        "import openpyxl\n",
        "from openpyxl import Workbook\n",
        "from openpyxl import load_workbook\n",
        "import pandas as pd\n",
        "import json\n",
        "from io import StringIO\n",
        "\n",
        "# load input file\n",
        "workbook = load_workbook(filename = 'llm_benchmark.xlsx', data_only=True) # data_only=True\n",
        "worksheet_emails = workbook['Emails'] # workbook.active\n",
        "worksheet_llm_config = workbook['LLM-Configs']\n",
        "\n",
        "# create log file (based on input file, sheet 'Results', all other sheets are deleted)\n",
        "workbook_log = load_workbook(filename = 'llm_benchmark.xlsx')\n",
        "workbook_log.remove(workbook_log['README'])\n",
        "workbook_log.remove(workbook_log['Emails'])\n",
        "workbook_log.remove(workbook_log['LLM-Configs'])\n",
        "worksheet_log = workbook_log['Results']\n",
        "while(worksheet_log.max_row > 1): # delete log rows: continuously delete row 2 until there is only first row (header row) left over\n",
        "  worksheet_log.delete_rows(2)\n",
        "\n",
        "# create headers dictionaries to access columns\n",
        "column_email = {}\n",
        "for column_index, cell in enumerate(worksheet_emails[\"1\"]):\n",
        "  column_email[cell.value] = column_index + 1\n",
        "column_llm_config = {}\n",
        "for column_index, cell in enumerate(worksheet_llm_config[\"1\"]):\n",
        "  column_llm_config[cell.value] = column_index + 1\n",
        "\n",
        "# iterate through all lines\n",
        "for row_index, row1 in enumerate(worksheet_emails):\n",
        "  if row_index == 12: # > 0  //  == 12\n",
        "    dateiname = worksheet_emails.cell(row=row_index+1, column=column_email[\"Dateiname\"]).value\n",
        "    if dateiname != None:\n",
        "      ic(dateiname)\n",
        "      prompt_context = worksheet_emails.cell(row=row_index+1, column=column_email[\"Prompt-Context\"]).value\n",
        "      target_JSON = worksheet_emails.cell(row=row_index+1, column=column_email[\"Ziel-JSON\"]).value\n",
        "      llm_configs = worksheet_emails.cell(row=row_index+1, column=column_email[\"LLM-Configs\"]).value.split(',')\n",
        "\n",
        "      for config_index, config in enumerate(llm_configs):\n",
        "        # search config\n",
        "        found_flag = False\n",
        "        for llm_config_index, row2 in enumerate(worksheet_llm_config):\n",
        "          llm_config_id = worksheet_llm_config.cell(row=llm_config_index+1, column=column_llm_config[\"LLM-Config-ID\"]).value\n",
        "          if llm_config_id.strip() == config.strip():\n",
        "            found_flag = True\n",
        "            break\n",
        "        if found_flag == True:\n",
        "          ic('LLM-Config found:', dateiname, config)\n",
        "          llm_config_LLM_Model = worksheet_llm_config.cell(row=llm_config_index+1, column=column_llm_config[\"LLM-Model\"]).value\n",
        "\n",
        "          # map LLM model names for calling by lite-llm\n",
        "          if llm_config_LLM_Model == 'Mixtral-8x7B':\n",
        "            llm_config_LLM_Model = 'together_ai/mistralai/Mixtral-8x7B-Instruct-v0.1'\n",
        "\n",
        "          llm_config_Prompt = worksheet_llm_config.cell(row=llm_config_index+1, column=column_llm_config[\"Prompt\"]).value\n",
        "          llm_config_Response_Format = worksheet_llm_config.cell(row=llm_config_index+1, column=column_llm_config[\"Response-Format\"]).value\n",
        "          prompt_full = llm_config_Prompt + '\\n\\n' + prompt_context\n",
        "          prompt_full_tokenlength = len(tiktoken.get_encoding('cl100k_base').encode(prompt_full))\n",
        "          #for x in prompt_full[:500].split('\\n'):\n",
        "          #  print(textwrap.fill(x, 80))\n",
        "\n",
        "          #\n",
        "          # do LLM call here\n",
        "          #\n",
        "          start_time = time.time()\n",
        "          response = completion(\n",
        "            #model=\"together_ai/mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "            model=llm_config_LLM_Model,\n",
        "            messages=[{ \"content\": prompt_full,\"role\": \"user\"}],\n",
        "            #temperature=0.5,\n",
        "            max_tokens=500\n",
        "          )\n",
        "          response_txt = response.choices[0].message.content\n",
        "          response_JSON = ''\n",
        "          if llm_config_Response_Format == \"JSON\":\n",
        "            response_JSON = response_txt\n",
        "          else:\n",
        "            if llm_config_Response_Format == \"CSV\":\n",
        "              response_IO = StringIO(response_txt) # [4:] strip away first 4 chars \"AI: \"\n",
        "              response_df = pd.read_csv(response_IO, sep=\"|\")\n",
        "              response_JSON_Obj = json.loads(response_df.to_json(orient='table',index=False))['data'][0]\n",
        "              response_JSON = json.dumps(response_JSON_Obj) # ,indent=4\n",
        "              response_JSON = (response_JSON.encode('latin-1').decode('unicode_escape')) # convert UNICODEs (caused by pd?)\n",
        "\n",
        "          # Fix some escaped characters\n",
        "          response_JSON = response_JSON.replace('\\_', '_')\n",
        "\n",
        "          response_model = response.model\n",
        "          response_responsetime = float(response._response_ms)/1000\n",
        "          time.sleep(1)\n",
        "          #response = \"response-obj\"\n",
        "          #response_txt = \"dummy text\"\n",
        "          stop_time = time.time()\n",
        "          elapsed_time = stop_time-start_time\n",
        "\n",
        "          # log result\n",
        "          current_date_time = datetime.now().strftime(\"%d.%m.%Y %H:%M:%S\")\n",
        "          # Time\tEmail-Name\n",
        "          log_row = [current_date_time, # Time\n",
        "                     dateiname,         # Email-Name\n",
        "                     config.strip(),    # LLM-Config-ID\n",
        "                     llm_config_LLM_Model, # Input-Model\n",
        "                     prompt_full,       # Input-Full-Prompt\n",
        "                     prompt_context,    # Input-Context\n",
        "                     str(response),     # Response-Full\n",
        "                     response_model,    # Response-Model\n",
        "                     response_responsetime, # Response-Duration\n",
        "                     elapsed_time,      # Duration-end-to-end\n",
        "                     response_txt,      # Ergebnis\n",
        "                     response_JSON,     # Ergebnis-JSON\n",
        "                     target_JSON         # Ziel-JSON\n",
        "\n",
        "          ]\n",
        "          worksheet_log.append(log_row)\n",
        "        else:\n",
        "          # LLM-Config not found - log result\n",
        "          ic('LLM-Config not found:', dateiname, config)\n",
        "          current_date_time = datetime.now().strftime(\"%d.%m.%Y %H:%M:%S\")\n",
        "          log_row = [current_date_time, dateiname,'NOT FOUND: ' + config.strip()]\n",
        "          worksheet_log.append(log_row)\n",
        "\n",
        "# Save the log file with timestamp\n",
        "current_date_time = '_' + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
        "current_date_time = \"\" # for debugging\n",
        "workbook_log.save('llm_benchmark_log' + current_date_time + '.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4TymQFZJw3V",
        "outputId": "e9bb0003-e323-49d6-be4a-88dd83d03076"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ic| dateiname: '24optimized.msg'\n",
            "ic| 'LLM-Config found:': 'LLM-Config found:'\n",
            "    dateiname: '24optimized.msg'\n",
            "    config: 'Simple_Prompt_1_GPT3_5'\n",
            "ic| 'LLM-Config found:': 'LLM-Config found:'\n",
            "    dateiname: '24optimized.msg'\n",
            "    config: ' Simple_Prompt_1_GPT4'\n",
            "ic| 'LLM-Config found:': 'LLM-Config found:'\n",
            "    dateiname: '24optimized.msg'\n",
            "    config: ' Simple_Prompt_1_Mixtral-8x7B'\n",
            "ic| 'LLM-Config found:': 'LLM-Config found:'\n",
            "    dateiname: '24optimized.msg'\n",
            "    config: ' KOR_Prompt_1_GPT3_5'\n",
            "ic| 'LLM-Config found:': 'LLM-Config found:'\n",
            "    dateiname: '24optimized.msg'\n",
            "    config: ' KOR_Prompt_1_GPT4'\n",
            "ic| 'LLM-Config found:': 'LLM-Config found:'\n",
            "    dateiname: '24optimized.msg'\n",
            "    config: ' KOR_Prompt_1_Mixtral-8x7B'\n",
            "ic| 'LLM-Config not found:': 'LLM-Config not found:'\n",
            "    dateiname: '24optimized.msg'\n",
            "    config: ' wrong_config '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate results"
      ],
      "metadata": {
        "id": "eEpsD3qZ0SbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openpyxl\n",
        "from openpyxl import Workbook\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "# load log file\n",
        "workbook_log = load_workbook(filename = 'llm_benchmark_log.xlsx') # data_only=True\n",
        "worksheet_log = workbook_log['Results']\n",
        "\n",
        "# create headers dictionaries to access columns\n",
        "column_log = {}\n",
        "for column_index, cell in enumerate(worksheet_log[\"1\"]):\n",
        "  column_log[cell.value] = column_index + 1\n",
        "ic(column_index)\n",
        "\n",
        "# iterate through all lines\n",
        "for log_index, row3 in enumerate(worksheet_log):\n",
        "  if log_index == 1: # > 0  //  == 1\n",
        "    log_time = worksheet_log.cell(row=log_index+1, column=column_log[\"Time\"]).value\n",
        "    ic('found', log_time)"
      ],
      "metadata": {
        "id": "Z2GGmbER3HVi",
        "outputId": "ba25be06-f676-4a82-ad83-0e0ad5b136bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ic| column_index: 12\n",
            "ic| 'found', log_time: '02.01.2024 15:35:52'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# simple LLM calls for testing"
      ],
      "metadata": {
        "id": "_N2G8OUkzXLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "client = openai.OpenAI(\n",
        "  api_key=os.environ.get(\"TOGETHERAI_API_KEY\"),\n",
        "  base_url='https://api.together.xyz',\n",
        ")\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Hello, how are you?\",\n",
        "    }\n",
        "  ],\n",
        "  model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "  max_tokens=1024\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku-bklBeaTpI",
        "outputId": "fa3551b9-545d-4ae1-9ebf-cbaa0da30eaa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I'm just a computer program, so I don't have feelings, but I'm here to help you with any language-related questions you have. Is there something specific you would like to know or practice?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call via lite-llm\n",
        "\n",
        "response = completion(\n",
        "  #model=\"together_ai/togethercomputer/Llama-2-7B-32K-Instruct\",\n",
        "  model=\"together_ai/mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "  messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoaFpFXKc8zE",
        "outputId": "d3e5ca89-7d3e-4e85-e855-c90a25b23fa5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to assist you with any questions you have. Is there something specific you would like to know or talk about?\n",
            "ModelResponse(id='chatcmpl-e90863b7-ef67-4899-a8d3-b28d85c9387f', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to assist you with any questions you have. Is there something specific you would like to know or talk about?\", role='assistant'))], created=1704384802, model='together_ai/mistralai/Mixtral-8x7B-Instruct-v0.1', object='chat.completion', system_fingerprint=None, usage=Usage(prompt_tokens=12, completion_tokens=44, total_tokens=56), _response_ms=1431.922)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = completion(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4QNQHwhc38u",
        "outputId": "0a0e02e8-51fa-45ca-d5e0-f0f62ba9745d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm an AI, so I don't have feelings, but I'm here to assist you. How can I help you today?\n",
            "ModelResponse(id='chatcmpl-8bPKKlY5fJEniH6li1bSHil8wYqAh', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"I'm an AI, so I don't have feelings, but I'm here to assist you. How can I help you today?\", role='assistant'))], created=1703926183, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=Usage(completion_tokens=27, prompt_tokens=13, total_tokens=40), _response_ms=1607.078)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function Calling\n",
        "\n",
        "# see https://litellm.vercel.app/docs/completion/function_call\n",
        "\n",
        "# via Huggingface?\n",
        "# https://litellm.vercel.app/docs/providers/huggingface\n",
        "# https://huggingface.co/Trelis/Mixtral-8x7B-Instruct-v0.1-function-calling-v3\n",
        "# https://huggingface.co/Trelis/Mistral-7B-Instruct-v0.1-function-calling-v2\n",
        "\n",
        "# via Anyscale?\n",
        "# https://docs.litellm.ai/docs/providers/anyscale\n",
        "# https://www.anyscale.com/blog/anyscale-endpoints-json-mode-and-function-calling-features\n",
        "\n",
        "import os, litellm\n",
        "from litellm import completion\n",
        "\n",
        "# IMPORTANT - Set this to TRUE to add the function to the prompt for Non OpenAI LLMs\n",
        "litellm.add_function_to_prompt = True\n",
        "\n",
        "# The real function is not needed for the LLM. It may be called after the LLM call (not in this code!)\n",
        "def get_current_weather(location):\n",
        "  if location == \"Boston, MA\":\n",
        "    return \"The weather is 12F\"\n",
        "\n",
        "functions = [\n",
        "    {\n",
        "      \"name\": \"get_current_weather\",\n",
        "      \"description\": \"Get the current weather in a given location\",\n",
        "      \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "          \"location\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
        "          },\n",
        "          \"unit\": {\n",
        "            \"type\": \"string\",\n",
        "            \"enum\": [\"celsius\", \"fahrenheit\"]\n",
        "          }\n",
        "        },\n",
        "        \"required\": [\"location\"]\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is the weather like in Boston?\"}\n",
        "]\n",
        "\n",
        "response = completion(model=\"gpt-3.5-turbo-1106\", messages=messages, functions=functions)\n",
        "\n",
        "print(response)\n",
        "print()\n",
        "function_found = hasattr(response.choices[0]['message'], 'function_call')\n",
        "if function_found == True:\n",
        "  function_call = response.choices[0]['message']['function_call']\n",
        "  function_call_name = function_call.name\n",
        "  function_call_arguments = function_call.arguments\n",
        "  print(function_call_name)\n",
        "else:\n",
        "  print('No function found')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TG6qj6W282Dz",
        "outputId": "03117e1e-99d0-4550-a0f5-7ffb14c908e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModelResponse(id='chatcmpl-8bPKfEySA6R1vl8rdfdB9cFJdphsh', choices=[Choices(finish_reason='function_call', index=0, message=Message(content=None, role='assistant', function_call=FunctionCall(arguments='{\"location\":\"Boston, MA\"}', name='get_current_weather')))], created=1703926205, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_772e8125bb', usage=Usage(completion_tokens=17, prompt_tokens=82, total_tokens=99), _response_ms=998.558)\n",
            "\n",
            "get_current_weather\n"
          ]
        }
      ]
    }
  ]
}